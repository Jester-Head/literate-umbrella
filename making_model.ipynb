{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import warnings\n",
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score,\\\n",
    "f1_score, roc_auc_score, roc_curve, precision_recall_curve,classification_report,label_ranking_average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from numba import jit, cuda\n",
    "import imblearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./input/traindata/updated_train_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to clean the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean the text performin some simple regex pattern matching\n",
    "def apply_regex(corpus):\n",
    "    corpus = corpus.apply(lambda x: re.sub(\"\\S*\\d\\S*\",\" \", x))          # removes numbers and words concatenated with numbers (IE h4ck3r)\n",
    "    corpus = corpus.apply(lambda x: re.sub(\"\\S*@\\S*\\s?\",\" \", x))        # removes emails and mentions (words with @)\n",
    "    corpus = corpus.apply(lambda x: re.sub(\"\\S*#\\S*\\s?\",\" \", x))        # removes hashtags (words with #)\n",
    "    corpus = corpus.apply(lambda x: re.sub(r'http\\S+', ' ', x))         # removes URLs\n",
    "    corpus = corpus.apply(lambda x: re.sub(r'[^a-zA-Z0-9 ]', ' ',x))    # keeps numbers and letters\n",
    "    corpus = corpus.apply(lambda x: x.replace(u'\\ufffd', '8'))          # replaces the ASCII 'ï¿½' symbol with '8'\n",
    "    corpus = corpus.apply(lambda x: re.sub(' +', ' ', x))               # removes multiple spaces\n",
    "    corpus.strip()\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function and clean the data\n",
    "feature = \"comment_text\"\n",
    "data[feature] = apply_regex(data[feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the following code is to demonstrate the data modification process of the original set. **Do not execute this section.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing orignal comment data\n",
    "comm = pd.read_csv('./input/traindata/traindata.csv',encoding='utf8',error_bad_lines=False,index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pol(df):\n",
    "    df['neg'] = df['comment_text'].apply(lambda comment_text:sid.polarity_scores(str(comment_text))['neg'])\n",
    "    df['neu'] = df['comment_text'].apply(lambda comment_text:sid.polarity_scores(str(comment_text))['neu'])\n",
    "    df['pos'] = df['comment_text'].apply(lambda comment_text:sid.polarity_scores(str(comment_text))['pos'])\n",
    "    df['compound'] = df['comment_text'].apply(lambda comment_text:sid.polarity_scores(str(comment_text))['compound'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit, cuda\n",
    "import numpy as np\n",
    "    \n",
    "# function optimized to run on gpu \n",
    "@jit(target_backend='cuda')    \n",
    "def sentiment_score(df):\n",
    "\tfor i,score in enumerate(df['compound']):\n",
    "\t\tif float(score) > 0:\n",
    "\t\t\tdf['sentiment'][i] = 'positive'\n",
    "\t\telif float(score) == 0.0:\n",
    "\t\t\tdf['sentiment'][i] = 'neutral'\n",
    "\t\telse:\n",
    "\t\t\tdf['sentiment'][i] ='negative'\n",
    "\treturn df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_pol(comm)\n",
    "df = sentiment_score(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './input/traindata/updated_train_data.csv'  \n",
    "df.to_csv(file_name, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Toxicity Rating Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[['comment_text','sentiment']]\n",
    "df['score'] = data['severe_toxic']+data['obscene']+data['threat']+data['insult']+data['identity_hate']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Text Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(df['sentiment'] +' '+ df['comment_text'])\n",
    "X.columns = ['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting and Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[['score']]\n",
    "X_train,X_test,y_train,y_test = train_test_split(X['text'],y,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodin text into vectors\n",
    "tfid = TfidfVectorizer(lowercase=False,max_features=500)\n",
    "\n",
    "train_vectors_tfidf = tfid.fit_transform(X_train).toarray()\n",
    "test_vectors_tfidf = tfid.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "train_vectors_tfidf, y_train_tfidf = oversample.fit_resample(train_vectors_tfidf, y_train.values.ravel())\n",
    "test_vectors_tfidf, y_test_tfidf = oversample.fit_resample(test_vectors_tfidf, y_test)\n",
    "\n",
    "\n",
    "\n",
    "norm_TFIDF = Normalizer(copy=False)\n",
    "norm_train_tfidf = norm_TFIDF.fit_transform(train_vectors_tfidf)\n",
    "norm_test_tfidf = norm_TFIDF.transform(test_vectors_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state=42, class_weight='balanced', n_jobs=6, max_iter=300)\n",
    "model.fit(norm_train_tfidf, y_train_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(norm_test_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_tfidf, prediction, labels=[0,1,2,3,4,5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'logist_model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ad924ab0e6f750209d62c6e0be45c7d7a480ad612e54686796527e723bb9d1f"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
